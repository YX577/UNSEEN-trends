---
title: "Predictive skill"
author: "Timo Kelder"
date: "October 19, 2019"
output: github_document 
---

```{r eval=FALSE, include=FALSE}
Rscript -e "rmarkdown::render('Predictive_skill.Rmd')"
```

In this notebook, we will first show the forecasts of the Norwegian West Coast compared to observed values and then we will assess the skill of the forecasts. 

## Import data and packages

```{r}
# dir='//home/timok/timok/SALIENSEAS/SEAS5/ensex'
# plotdir=paste0(dir,'/statistics/multiday/plots')
# dir='/home/timok/ensex'
# plotdir='/home/timok/Documents/ensex/R/graphs'
dir='C:/Users/gytk3/OneDrive - Loughborough University/GitHub/EnsEx/Data'

source('Load_data.R')
```

##SEAS5 compared to SeNorge

Plot the observations and the forecasts for: 

- The raw values for the observations and the forecasts
- The mean bias corrected forecasts
- The standardized anomaly for the forecasts and the observations 


```{r fig1, fig.height = 9, fig.width = 5, fig.align = "center"}
par(mfrow=c(3,1))
plot(1981:2015,predictand,type='l',ylim=c(0,175), ylab = 'SON three-day precipitation maxima',xlab = '')
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,Extremes_WC[mbr,ld,],col=alpha('blue',0.1))}}
axis(side = 1,1981:2015,labels = F)


plot(1981:2015,predictand-mean(predictand),type='l',ylim=c(-60,80),ylab = 'Anomalies',xlab = '')
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,Extremes_WC[mbr,ld,]-mean(Extremes_WC),col=alpha('blue',0.1))}}
axis(side = 1,1981:2015,labels = F)

plot(1981:2015,predictand_anomaly,type='l',ylim = c(-2.5,4),ylab='Standardized anomalies',xlab = '')
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,calc_anomaly(Extremes_WC[mbr,ld,]),col=alpha('blue',0.1))}}
axis(side = 1,1981:2015,labels = F)

```

To compare the obervation with the simulations, we show the rank histogram for each of the three plots. The rank histograms illustrates on what rank the observations lie compared to all forecasts. If the simulations are not biased and represent plausible realizations of reality, the observations would be randomly distributed over the range of simulations and the rank histogram would be flat. 

From both the time series and histogram plots, it is clear that the forecast have a lower bias. When corrected for this mean bias, the ranks of the observations seem to be equally distributed over the forecast ranks.   

```{r}
rank.histogram <- function(pred,obs=NULL) {
  
  N <- dim(pred)[2]
  K <- dim(pred)[1]
  
  ranks <- apply(rbind(obs, pred), 2, rank, ties.method="random")[1, ]
  rank.hist <- hist(ranks, breaks=seq(-2.5, K+2.5,5))[["counts"]]
}

# apply(pred, MARGIN = 3, rank.histogram) #What am I doing wrong?

rank.histogram(rbind(Extremes_WC[,1,],Extremes_WC[,2,],Extremes_WC[,3,],Extremes_WC[,4,]),predictand)
rank.histogram(rbind(Extremes_WC[,1,],Extremes_WC[,2,],Extremes_WC[,3,],Extremes_WC[,4,])-mean(Extremes_WC),predictand-mean(predictand))

pred=apply(Extremes_WC,MARGIN = c(1,2) , FUN=calc_anomaly)

rank.histogram <- function(pred,obs=NULL) {
  N <- dim(pred)[1]
  K <- dim(pred)[2]
  
  ranks <- apply(cbind(obs, pred), 1, rank, ties.method="random")[1, ]
  rank.hist <- hist(ranks, breaks=seq(-2.5, K+2.5,5))[["counts"]]
}
rank.histogram(cbind(pred[,,1],pred[,,2],pred[,,3],pred[,,4]),predictand_anomaly)

```

## Predictive skill
Then, for the predictive skill, the mean of the forecasts for each leadtime is used for the raw and standardized data.

```{r}
plot(1981:2015,predictand,type='l')
lines(1981:2015,predictor_all,col='orange')
lines(1981:2015,predictor['2',],col='blue')
lines(1981:2015,predictor['3',],col='blue')
lines(1981:2015,predictor['4',],col='blue')
lines(1981:2015,predictor['5',],col='blue')

plot(1981:2015,predictand_anomaly,type='l')
lines(1981:2015,predictor_anomaly_all,col='orange')
lines(1981:2015,predictor_anomaly[,'2'],col='blue')
lines(1981:2015,predictor_anomaly[,'3'],col='blue')
lines(1981:2015,predictor_anomaly[,'4'],col='blue')
lines(1981:2015,predictor_anomaly[,'5'],col='blue')
```

We compare the anomaly of the observations with forecasts for each of the lead times. There is not much of a correlation and the lead times are not significantly different.  
```{r}

df <- data.frame(predictand_anomaly,predictor_anomaly) %>%
  # select(predictand_anomaly, X2,X3,X4,X5) %>%
  gather(key = "variable", value = "value", -predictand_anomaly)
df_all <- data.frame(predictand_anomaly,predictor_anomaly_all)

ggplot() +
  geom_point(data=df, aes(x=predictand_anomaly, y=value, color=variable, shape=variable)) +
  geom_smooth(method=lm,data=df, aes(x=predictand_anomaly, y=value, color=variable,fill=variable))+
  geom_point(data=df_all, aes(x=predictand_anomaly, y=predictor_anomaly_all), color='black') + 
  geom_smooth(method=lm,data=df_all, aes(x=predictand_anomaly, y=predictor_anomaly_all), color='black')+
  xlim(-2.5, 2.5) +
  ylim(-2.5, 2.5) +
  theme_classic() 



```


We calculate the correlation test between the anomalies of the ensemble mean of each lead time and the observations

```{r}
#Use spearman to avoid normality assumptions
cor_coeff='spearman'
correlation_test <- function(predictor_anomaly) {
  
correlation=cor.test(predictand_anomaly,predictor_anomaly,alternative = 'two.sided',method = cor_coeff) #alternative hypothesis is that the population correlation is greater than 0. -> we don't expect negative correlations? 
return(c(correlation$estimate,correlation$p.value))}# 
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
print('For the pooled leadtimes')
correlation_test(predictor_anomaly = predictor_anomaly_all)

```

The correlations are very small. Does it even make sense to continue and calculate the ratio of predictable components? I don't think so. The main question I have is: Do we trust the trends in the anomalies of the model to be representative of trends in reality (the anomalies of the observations), even if there is no skill? See the forecast reliability notebook. 


A little extra: What about the correlation between the different leadtimes? They also have very small correlations

```{r}
predictand_anomaly=predictor_anomaly[,'2']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'3']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'4']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'5']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)


```

##The ration of predictable components (RPC).
This can be calculated in two ways: 1) the variation of the mean compared to the variation of single members and 2) the RMSE compared to the variation.

```{r}
Var_signal=sd(predictor_all)^2 #The signal is the interannual variance of the model mean

sd_ensemble=apply(Extremes_WC,MARGIN = c(1,2),FUN=sd) #predictor['2','1987']
Var_noise=mean(sd_ensemble)^2 #The noise is the variance of the individual ensemble members about the ensemble mean


PC_mod=Var_signal/(Var_signal+Var_noise)# Intrinsic model predictability of the signal

correlation=cor.test(predictand,predictor_all,alternative = 'two.sided',method = 'pearson') #alternative hypothesis is that the population correlation is greater than 0. -> we don't expect negative correlations? 
PC_reality=correlation$estimate #the predictability of the model 
RPC1= PC_reality/sqrt(PC_mod)

RMSE = sqrt(mean((predictor_all - predictand)^2))
RMSE_biascor = sqrt(mean((predictor_all*(mean(predictand)/mean(predictor)) - predictand)^2))
RPC2=RMSE/mean(sd_ensemble)


```


