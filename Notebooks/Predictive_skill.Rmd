---
title: "Predictive skill"
author: "Timo Kelder"
date: "October 19, 2019"
output: github_document 
---

```{r eval=FALSE, include=FALSE}
Rscript -e "rmarkdown::render('Predictive_skill.Rmd')"
```

## Import data and packages

```{r}
dir='//home/timok/timok/SALIENSEAS/SEAS5/ensex'
plotdir=paste0(dir,'/statistics/multiday/plots')
source('Load_data.R')
```

##Visualize the data

Plot the observations and the forecasts for: 

- The raw values for the observations and the forecasts
- The mean bias corrected forecasts
- The standardized anomaly for the forecasts and the observations 


```{r}
plot(1981:2015,predictand,type='l',ylim=c(0,175))
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,Extremes_WC[mbr,ld,],col=alpha('blue',0.1))}}

plot(1981:2015,predictand-mean(predictand),type='l',ylim=c(-60,80))
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,Extremes_WC[mbr,ld,]-mean(Extremes_WC),col=alpha('blue',0.1))}}

plot(1981:2015,predictand_anomaly,type='l',ylim = c(-2.5,4))
for (mbr in 1:25){
  for (ld in 1:4){
    lines(1981:2015,calc_anomaly(Extremes_WC[mbr,ld,]),col=alpha('blue',0.1))}}

```

To compare the obervation with the simulations, we show the rank histogram for each of the three plots. The rank histograms illustrates on what rank the observations lie compared to all forecasts. If the simulations are not biased and represent plausible realizations of reality, the observations would be randomly distributed over the range of simulations and the rank histogram would be flat. 

From both the time series and histogram plots, it is clear that the forecast have a lower bias. When corrected for this mean bias, the ranks of the observations seem to be equally distributed over the forecast ranks.   

```{r}
rank.histogram <- function(pred,obs=NULL) {
  
  N <- dim(pred)[2]
  K <- dim(pred)[1]
  
  ranks <- apply(rbind(obs, pred), 2, rank, ties.method="random")[1, ]
  rank.hist <- hist(ranks, breaks=seq(-2.5, K+2.5,5))[["counts"]]
}

# apply(pred, MARGIN = 3, rank.histogram) #What am I doing wrong?

rank.histogram(rbind(Extremes_WC[,1,],Extremes_WC[,2,],Extremes_WC[,3,],Extremes_WC[,4,]),predictand)
rank.histogram(rbind(Extremes_WC[,1,],Extremes_WC[,2,],Extremes_WC[,3,],Extremes_WC[,4,])-mean(Extremes_WC),predictand-mean(predictand))

pred=apply(Extremes_WC,MARGIN = c(1,2) , FUN=calc_anomaly)

rank.histogram <- function(pred,obs=NULL) {
  N <- dim(pred)[1]
  K <- dim(pred)[2]
  
  ranks <- apply(cbind(obs, pred), 1, rank, ties.method="random")[1, ]
  rank.hist <- hist(ranks, breaks=seq(-2.5, K+2.5,5))[["counts"]]
}
rank.histogram(cbind(pred[,,1],pred[,,2],pred[,,3],pred[,,4]),predictand_anomaly)

```

Then, for the predictive skill, the mean of the forecasts for each leadtime is used for the raw and standardized data.

```{r}
plot(1981:2015,predictand,type='l')
lines(1981:2015,predictor['2',],col='blue')
lines(1981:2015,predictor['3',],col='blue')
lines(1981:2015,predictor['4',],col='blue')
lines(1981:2015,predictor['5',],col='blue')

plot(1981:2015,predictand_anomaly,type='l')
lines(1981:2015,predictor_anomaly[,'2'],col='blue')
lines(1981:2015,predictor_anomaly[,'3'],col='blue')
lines(1981:2015,predictor_anomaly[,'4'],col='blue')
lines(1981:2015,predictor_anomaly[,'5'],col='blue')
```

We compare the anomaly of the observations with forecasts for each of the lead times. There is not much of a correlation and the lead times are not significantly different.  
```{r}

df <- data.frame(predictand_anomaly,predictor_anomaly) %>%
  select(predictand_anomaly, X2,X3,X4,X5) %>%
  gather(key = "variable", value = "value", -predictand_anomaly)

p= ggplot(df, aes(x=predictand_anomaly, y=value, color=variable, shape=variable)) +
  geom_point() + 
  geom_smooth(method=lm)+
  xlim(-2.5, 2.5) +
  ylim(-2.5, 2.5) +
  theme_classic() 

# p1 = p + scale_color_grey()
p2 = p + scale_color_brewer(palette="Dark2")
# p1
p2


```


##Correlations
Calculate the correlation test between the anomalies of the ensemble mean of each lead time and the observations

```{r}
#Use spearman to avoid normality assumptions
cor_coeff='spearman'
correlation_test <- function(predictor_anomaly) {
  
correlation=cor.test(predictand_anomaly,predictor_anomaly,alternative = 'two.sided',method = cor_coeff) #alternative hypothesis is that the population correlation is greater than 0. -> we don't expect negative correlations? 
return(c(correlation$estimate,correlation$p.value))}# 
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
```

The correlations are very small. Does it even make sense to continue and calculate the ratio of predictable components? I don't think so. The main question I have is: Do we trust the trends in the anomalies of the model to be representative of trends in reality (the anomalies of the observations), even if there is no skill? See the forecast reliability notebook. 


A little extra: What about the correlation between the different leadtimes? They also have very small correlations

```{r}
predictand_anomaly=predictor_anomaly[,'2']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'3']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'4']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)
predictand_anomaly=predictor_anomaly[,'5']
apply(predictor_anomaly,MARGIN = 2 , FUN=correlation_test)


```






